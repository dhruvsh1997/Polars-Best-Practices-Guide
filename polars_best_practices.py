# -*- coding: utf-8 -*-
"""Polars Best Practices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1id6hTXBZGKqS3b33hk5jn2kIFh5clrja

# üìå Introduction to Polars
Polars is a **blazing-fast DataFrame library** written in Rust and designed for **performance, parallelism**, and **memory efficiency**. It provides both eager and lazy APIs, supports Arrow data formats, and is especially suitable for large datasets.

We'll now explore Polars step by step with practical use cases.
"""

# üì¶ Install Polars
!pip install polars[all] --quiet

import polars as pl
print(pl.__version__)

# ‚úÖ Importing the Library
import polars as pl

# ‚ñ∂Ô∏è Create DataFrame
df = pl.DataFrame({
    "col1": [1, 2, 3],
    "col2": ["x", "y", "z"]
})
df

"""üì• Reading Data"""

from sklearn.datasets import make_regression
import pandas as pd
# Create synthetic finance-related data
# Let's imagine features like 'stock_price_yesterday', 'interest_rate', 'inflation_rate',
# 'volume', 'market_sentiment', and a target 'stock_price_today'.
X, y = make_regression(n_samples=300, n_features=10, random_state=42, n_informative=8, noise=10)

# Convert to pandas DataFrame
column_names = [f'feature_{i+1}' for i in range(10)]
df_finance = pd.DataFrame(X, columns=column_names)
df_finance['target_stock_price'] = y

# Add some columns that might be more finance-specific (even if synthetic)
df_finance['stock_price_yesterday'] = df_finance['feature_1'] * 100 + 500 # Example calculation
df_finance['interest_rate'] = df_finance['feature_2'] * 0.1 + 2.0 # Example calculation
df_finance['inflation_rate'] = df_finance['feature_3'] * 0.05 + 1.0 # Example calculation

# Drop some of the original 'feature' columns to keep it around 10
df_finance = df_finance[['stock_price_yesterday', 'interest_rate', 'inflation_rate',
                         'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8',
                         'feature_9', 'target_stock_price']]

# Save to CSV
csv_file_path = 'finance_data.csv'
df_finance.to_csv(csv_file_path, index=False)
# Save to CSV
csv_file_path = 'finance_data.xlsx'
df_finance.to_excel(csv_file_path, index=False)
#Save to DB
import sqlite3
conn = sqlite3.connect('finance_data.db')
# Save the pandas DataFrame to a SQLite table
df_finance.to_sql('finance_data', conn, if_exists='replace', index=False)
conn.close()
print("DataFrame 'df_finance' successfully saved to 'finance_data.db' as table 'finance_data'.")

print(f"Synthetic finance data saved to {csv_file_path}")
print(df_finance.head())

#/content/finance_data.csv
# üìÅ Read CSV
df_csv = pl.read_csv("/content/finance_data.csv")
df_csv.head()

# üìÅ Read Excel (requires optional dependency)
df_excel = pl.read_excel("/content/finance_data.xlsx")
df_excel.head()

# üõ¢Ô∏è Read from Database (simulate with SQLite for example)
import sqlite3
conn = sqlite3.connect("/content/finance_data.db")
df_db = pl.read_database("SELECT * FROM finance_data", connection=conn)
df_db.head()

"""üíæ Writing Data"""

# üíæ Write to CSV
df_csv.write_csv("/content/finance_data.csv")

# üíæ Write to Excel
df_excel.write_excel("/content/finance_data.xlsx")

# üíæ Write to Database
# df_db.write_database(table_name="finance_data", connection=conn)

"""üëÄ View & Inspect Data"""

# üîç View Top/Bottom Rows
df_csv.head(5)

df_csv.tail(5)

# üß¨ Schema Information
df_csv.schema

# üìä Statistical Summary
df_csv.describe()

# üìê Dimensions
df_csv.height, df_csv.width

# üìã Column Names
df_csv.columns

"""üßπ Handling Nulls / Missing Data"""

# ‚ùì Count Nulls
df_csv.null_count()

# ‚ùå Drop Nulls
df_csv.drop_nulls()

# üîÑ Fill Nulls
df_csv.fill_null(value=0)

"""üéØ Selecting Data"""

# üìå Select Columns
df_csv["stock_price_yesterday"]
df_csv.select(["feature_4", "feature_5"])

# üéØ Select Rows
df_csv.slice(1, 2)  # start=1, length=2

#Supportive Code
import numpy as np
from datetime import datetime, timedelta

# Add 'forecast_date' column (random dates within a range)
start_date = datetime.now() - timedelta(days=365) # Start one year ago
end_date = datetime.now() + timedelta(days=365)   # End one year from now

random_dates = [start_date + timedelta(days=np.random.randint(0, (end_date - start_date).days)) for _ in range(df_csv.height)]
df_csv = df_csv.with_columns(
    pl.Series("forecast_date", [date.strftime("%Y-%m-%d") for date in random_dates])
)

# Add 'type' column (randomly 'High' or 'Low')
random_types = np.random.choice(['High', 'Low'], size=df_csv.height)
df_csv = df_csv.with_columns(
    pl.Series("type", random_types.tolist())
)

print(df_csv.head())

# üîç Filtering
df_csv.filter(pl.col("feature_5") > 0)
df_csv.filter(pl.col("type").str.contains("Low"))

"""üßº Data Cleaning / Transformation"""

# üìÜ Convert to Datetime
df_date = df_csv.with_columns(pl.col("forecast_date").str.to_date())
# Display the head of the new DataFrame to confirm the change
df_date.head()

# üîÄ Type Conversion Int
df_csv.with_columns(pl.col("stock_price_yesterday").cast(pl.Int16))

# üîÄ Type Conversion Float
df_csv.with_columns(pl.col("stock_price_yesterday").cast(pl.Float32))

# üî† String Lowercase
df_csv.with_columns(pl.col("type").str.to_lowercase())

# üöø Strip Characters
df_csv.with_columns(pl.col("type").str.strip_chars())

# üîÅ Replace Strings
df_csv.with_columns(pl.col("type").str.replace("w", "ww"))

"""üÜé Rename & Create Columns"""

# üè∑Ô∏è Rename Columns
df_csv.rename({"type": "TYPE"})

# ‚ûï New Column
df.with_columns((pl.col("col1") * 2).alias("col1_double"))

# üß† Apply Function
df.with_columns(
    pl.col("col1").map_elements(lambda x: x + 10).alias("plus_10")
)

"""üßÆ GroupBy & Aggregation"""

# üìä Group and Aggregate
df_group = pl.DataFrame({
    "col": ["a", "a", "b", "b"],
    "value": [10, 20, 30, 40]
})
df_group.group_by("col").agg(pl.col("value").mean().alias("mean_val"))

"""üîó Join and Concatenate"""

# üîó Join
df1 = pl.DataFrame({"id": [1, 2], "val": ["x", "y"]})
df2 = pl.DataFrame({"id": [1, 2], "score": [100, 200]})
df1.join(df2, on="id", how="inner")

# üìé Concatenate
pl.concat([df1, df1], how="vertical")

"""üìä Pivot and Unpivot"""

# üîÑ Pivot
df_pivot = pl.DataFrame({
    "city": ["NY", "NY", "LA", "LA"],
    "department": ["HR", "Tech", "HR", "Tech"],
    "salary": [1000, 1500, 1100, 1600]
})
df_pivot.pivot(index="city", columns="department", values="salary", aggregate_function="mean")

# üîÅ Unpivot
# Changed id_vars to on and value_vars to index to match Polars unpivot syntax
df_pivot.unpivot(on=["city", "department"], index=["salary"])

"""‚è±Ô∏è Time-Based Operations"""

# üìÜ Datetime Extraction
# Step 1: Generate the date range correctly using `start` and `end`
# date_range_df = pl.select(
#     pl.date_range(start="2024-01-01", end="2024-06-01", interval="1mo", name="date")
# )

# # Step 2: Extract the year from the date
# df_dt = date_range_df.with_columns(
#     pl.col("date").dt.year().alias("year")
# )

# df_dt

# # üïí Dynamic GroupBy
# df_time = pl.DataFrame({
#     "date": pl.date_range("2024-01-01", "2024-03-01", interval="1d"),
#     "value": range(60)
# })
# df_time.group_by_dynamic("date", every="1mo").agg(pl.col("value").sum())

# # üìâ Rolling Window
# df_time.rolling(index_column="date", period="7d").agg(pl.col("value").mean())

"""üî¢ Counts and Frequency"""

# üìä Value Counts
df_csv.group_by("type").len()

df_csv["type"].value_counts()

"""üì¶ Batch / Lazy Evaluation"""

# üöö Read CSV in Chunks
for df_chunk in pl.read_csv("/content/finance_data.csv", batch_size=10000):
    print(df_chunk.shape)

# üí§ Lazy Evaluation
pl.scan_csv("/content/finance_data.csv").filter(pl.col("feature_9") > 0).collect()

"""üîÑ Category Encoding and Vector Ops"""

# üè∑Ô∏è Category
df_csv.with_columns(pl.col("type").cast(pl.Categorical))

# ‚ö° Vectorized Operations
df.with_columns((pl.col("col1") * 3).alias("tripled"))

"""# ‚úÖ Summary of Polars Strengths
- **Speed:** Built in Rust, parallel execution by default.
- **Memory-efficient:** Lazy API uses minimal RAM.
- **Built-in Lazy Queries** for pipeline-style processing.
- **Excellent String & Date support**.
- **Arrow and Parquet integration** for efficient file operations.

üìå Use Polars when dealing with:
- Huge CSVs or structured data.
- Memory-sensitive environments.
- Data transformations with high performance requirements.

---
Now you‚Äôre equipped to use Polars as a modern, high-performance DataFrame engine. Start replacing slow pandas pipelines with Polars for 10x performance boosts!

"""



"""
Act as a Python Expert who have worked on various advance concepts of Polars Libraries in Python with various Real life like use cases. now I am learning dataframe management and already learn pandas and in that I have seen polars lybraries alternative of of pandas with same use case time to time, so I though I should Learn polars for myself. Now as someone who never know anything about Polars. help me understand and learn best practices of polars library used on regular bases, and also help me learn how its useful while using in different use case like: -
DataFrame Creation: pl.DataFrame({"col1": [1, 2, 3], "col2": ["x", "y", "z"]})
Read CSV: pl.read_csv("file.csv")
Read Excel: pl.read_excel("file.xlsx")
Read Database: pl.read_database(query="SELECT * FROM table", connection=conn)
Write CSV: df.write_csv("output.csv")
Write Excel: df.write_excel("output.xlsx")
Write Database: df.write_database(table_name="table", connection=conn)
View First Rows: df.head(n=5)
View Last Rows: df.tail(n=5)
Schema Information: df.schema
Statistical Summary: df.describe()
Dimensions: df.height, df.width
List Columns: df.columns
Count Missing Values: df.null_count()
Drop Missing Values: df.drop_nulls()
Fill Missing Values: df.fill_null(value=0)
Select Single Column: df["column_name"]
Select Multiple Columns: df.select(["col1", "col2"])
Select Rows by Position: df.slice(start, length)
Filter Rows: df.filter(pl.col("col") > value)
String Contains Filter: df.filter(pl.col("col").str.contains("pattern"))
Drop Duplicates: df.unique() or df.unique(subset=["col1"])
Convert to Datetime: df.with_columns(pl.col("date").str.to_datetime())
Convert Data Type: df.with_columns(pl.col("col").cast(pl.Int32))
String Lowercase: df.with_columns(pl.col("col").str.to_lowercase())
String Strip: df.with_columns(pl.col("col").str.strip_chars())
String Replace: df.with_columns(pl.col("col").str.replace("old", "new"))
Rename Columns: df.rename({"old_name": "new_name"})
Create New Column: df.with_columns(pl.col("col") * 2.0.alias("new_col"))
Apply Function: df.with_columns(pl.col("col").map_elements(lambda x: func(x)))
GroupBy and Aggregate: df.group_by("col").agg(pl.col("value").mean().alias("mean_value"))
Join DataFrames: df.join(df2, on="key_column", how="inner")
Concatenate DataFrames: pl.concat([df1, df2], how="vertical")
Pivot Table: df.pivot(index="city", columns="department", values="salary", aggregate_function="mean")
Unpivot (Melt): df.unpivot(index="city", on=["salary", "bonus"])
Datetime Extraction: df.with_columns(pl.col("date").dt.year().alias("year"))
Dynamic GroupBy (Resampling): df.group_by_dynamic("date", every="1mo").agg(pl.col("value").sum())
Rolling Window: df.rolling(index_column="date", period="3d").agg(pl.col("value").mean())
Value Counts: df.group_by("col").len() or df["col"].value_counts()
Read in Chunks: pl.read_csv("file.csv", batch_size=10000)
Lazy Evaluation: pl.scan_csv("file.csv").filter(pl.col("age") > 25).collect()
Category Data Type: df.with_columns(pl.col("col").cast(pl.Categorical))
Vectorized Operations: pl.col("col") * 2 (used within with_columns or select), All I want to say is keep the examples diverse for different component seperate for all the components I wrote with polars, with polas's strength with its most used concept or practices of pydantic. Provide Cell by cell code and markdowns(appropriate to colab) I will copy paste it myself. Can Search on web for details.
"""

