# -*- coding: utf-8 -*-
"""Polars Best Practices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1id6hTXBZGKqS3b33hk5jn2kIFh5clrja

# ðŸ“Œ Introduction to Polars
Polars is a **blazing-fast DataFrame library** written in Rust and designed for **performance, parallelism**, and **memory efficiency**. It provides both eager and lazy APIs, supports Arrow data formats, and is especially suitable for large datasets.

We'll now explore Polars step by step with practical use cases.
"""

# ðŸ“¦ Install Polars
!pip install polars[all] --quiet

import polars as pl
print(pl.__version__)

# âœ… Importing the Library
import polars as pl

# â–¶ï¸ Create DataFrame
df = pl.DataFrame({
    "col1": [1, 2, 3],
    "col2": ["x", "y", "z"]
})
df

"""ðŸ“¥ Reading Data"""

from sklearn.datasets import make_regression
import pandas as pd
# Create synthetic finance-related data
# Let's imagine features like 'stock_price_yesterday', 'interest_rate', 'inflation_rate',
# 'volume', 'market_sentiment', and a target 'stock_price_today'.
X, y = make_regression(n_samples=300, n_features=10, random_state=42, n_informative=8, noise=10)

# Convert to pandas DataFrame
column_names = [f'feature_{i+1}' for i in range(10)]
df_finance = pd.DataFrame(X, columns=column_names)
df_finance['target_stock_price'] = y

# Add some columns that might be more finance-specific (even if synthetic)
df_finance['stock_price_yesterday'] = df_finance['feature_1'] * 100 + 500 # Example calculation
df_finance['interest_rate'] = df_finance['feature_2'] * 0.1 + 2.0 # Example calculation
df_finance['inflation_rate'] = df_finance['feature_3'] * 0.05 + 1.0 # Example calculation

# Drop some of the original 'feature' columns to keep it around 10
df_finance = df_finance[['stock_price_yesterday', 'interest_rate', 'inflation_rate',
                         'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8',
                         'feature_9', 'target_stock_price']]

# Save to CSV
csv_file_path = 'finance_data.csv'
df_finance.to_csv(csv_file_path, index=False)
# Save to CSV
csv_file_path = 'finance_data.xlsx'
df_finance.to_excel(csv_file_path, index=False)
#Save to DB
import sqlite3
conn = sqlite3.connect('finance_data.db')
# Save the pandas DataFrame to a SQLite table
df_finance.to_sql('finance_data', conn, if_exists='replace', index=False)
conn.close()
print("DataFrame 'df_finance' successfully saved to 'finance_data.db' as table 'finance_data'.")

print(f"Synthetic finance data saved to {csv_file_path}")
print(df_finance.head())

#/content/finance_data.csv
# ðŸ“ Read CSV
df_csv = pl.read_csv("/content/finance_data.csv")
df_csv.head()

# ðŸ“ Read Excel (requires optional dependency)
df_excel = pl.read_excel("/content/finance_data.xlsx")
df_excel.head()

# ðŸ›¢ï¸ Read from Database (simulate with SQLite for example)
import sqlite3
conn = sqlite3.connect("/content/finance_data.db")
df_db = pl.read_database("SELECT * FROM finance_data", connection=conn)
df_db.head()

"""ðŸ’¾ Writing Data"""

# ðŸ’¾ Write to CSV
df_csv.write_csv("/content/finance_data.csv")

# ðŸ’¾ Write to Excel
df_excel.write_excel("/content/finance_data.xlsx")

# ðŸ’¾ Write to Database
# df_db.write_database(table_name="finance_data", connection=conn)

"""ðŸ‘€ View & Inspect Data"""

# ðŸ” View Top/Bottom Rows
df_csv.head(5)

df_csv.tail(5)

# ðŸ§¬ Schema Information
df_csv.schema

# ðŸ“Š Statistical Summary
df_csv.describe()

# ðŸ“ Dimensions
df_csv.height, df_csv.width

# ðŸ“‹ Column Names
df_csv.columns

"""ðŸ§¹ Handling Nulls / Missing Data"""

# â“ Count Nulls
df_csv.null_count()

# âŒ Drop Nulls
df_csv.drop_nulls()

# ðŸ”„ Fill Nulls
df_csv.fill_null(value=0)

"""ðŸŽ¯ Selecting Data"""

# ðŸ“Œ Select Columns
df_csv["stock_price_yesterday"]
df_csv.select(["feature_4", "feature_5"])

# ðŸŽ¯ Select Rows
df_csv.slice(1, 2)  # start=1, length=2

#Supportive Code
import numpy as np
from datetime import datetime, timedelta

# Add 'forecast_date' column (random dates within a range)
start_date = datetime.now() - timedelta(days=365) # Start one year ago
end_date = datetime.now() + timedelta(days=365)   # End one year from now

random_dates = [start_date + timedelta(days=np.random.randint(0, (end_date - start_date).days)) for _ in range(df_csv.height)]
df_csv = df_csv.with_columns(
    pl.Series("forecast_date", [date.strftime("%Y-%m-%d") for date in random_dates])
)

# Add 'type' column (randomly 'High' or 'Low')
random_types = np.random.choice(['High', 'Low'], size=df_csv.height)
df_csv = df_csv.with_columns(
    pl.Series("type", random_types.tolist())
)

print(df_csv.head())

# ðŸ” Filtering
df_csv.filter(pl.col("feature_5") > 0)
df_csv.filter(pl.col("type").str.contains("Low"))

"""ðŸ§¼ Data Cleaning / Transformation"""

# ðŸ“† Convert to Datetime
df_date = df_csv.with_columns(pl.col("forecast_date").str.to_date())
# Display the head of the new DataFrame to confirm the change
df_date.head()

# ðŸ”€ Type Conversion Int
df_csv.with_columns(pl.col("stock_price_yesterday").cast(pl.Int16))

# ðŸ”€ Type Conversion Float
df_csv.with_columns(pl.col("stock_price_yesterday").cast(pl.Float32))

# ðŸ”  String Lowercase
df_csv.with_columns(pl.col("type").str.to_lowercase())

# ðŸš¿ Strip Characters
df_csv.with_columns(pl.col("type").str.strip_chars())

# ðŸ” Replace Strings
df_csv.with_columns(pl.col("type").str.replace("w", "ww"))

"""ðŸ†Ž Rename & Create Columns"""

# ðŸ·ï¸ Rename Columns
df_csv.rename({"type": "TYPE"})

# âž• New Column
df.with_columns((pl.col("col1") * 2).alias("col1_double"))

# ðŸ§  Apply Function
df.with_columns(
    pl.col("col1").map_elements(lambda x: x + 10).alias("plus_10")
)

"""ðŸ§® GroupBy & Aggregation"""

# ðŸ“Š Group and Aggregate
df_group = pl.DataFrame({
    "col": ["a", "a", "b", "b"],
    "value": [10, 20, 30, 40]
})
df_group.group_by("col").agg(pl.col("value").mean().alias("mean_val"))

"""ðŸ”— Join and Concatenate"""

# ðŸ”— Join
df1 = pl.DataFrame({"id": [1, 2], "val": ["x", "y"]})
df2 = pl.DataFrame({"id": [1, 2], "score": [100, 200]})
df1.join(df2, on="id", how="inner")

# ðŸ“Ž Concatenate
pl.concat([df1, df1], how="vertical")

"""ðŸ“Š Pivot and Unpivot"""

# ðŸ”„ Pivot
df_pivot = pl.DataFrame({
    "city": ["NY", "NY", "LA", "LA"],
    "department": ["HR", "Tech", "HR", "Tech"],
    "salary": [1000, 1500, 1100, 1600]
})
df_pivot.pivot(index="city", columns="department", values="salary", aggregate_function="mean")

# ðŸ” Unpivot
# Changed id_vars to on and value_vars to index to match Polars unpivot syntax
df_pivot.unpivot(on=["city", "department"], index=["salary"])

"""â±ï¸ Time-Based Operations"""

# ðŸ“† Datetime Extraction
# Step 1: Generate the date range correctly using `start` and `end`
# date_range_df = pl.select(
#     pl.date_range(start="2024-01-01", end="2024-06-01", interval="1mo", name="date")
# )

# # Step 2: Extract the year from the date
# df_dt = date_range_df.with_columns(
#     pl.col("date").dt.year().alias("year")
# )

# df_dt

# # ðŸ•’ Dynamic GroupBy
# df_time = pl.DataFrame({
#     "date": pl.date_range("2024-01-01", "2024-03-01", interval="1d"),
#     "value": range(60)
# })
# df_time.group_by_dynamic("date", every="1mo").agg(pl.col("value").sum())

# # ðŸ“‰ Rolling Window
# df_time.rolling(index_column="date", period="7d").agg(pl.col("value").mean())

"""ðŸ”¢ Counts and Frequency"""

# ðŸ“Š Value Counts
df_csv.group_by("type").len()

df_csv["type"].value_counts()

"""ðŸ“¦ Batch / Lazy Evaluation"""

# ðŸšš Read CSV in Chunks
for df_chunk in pl.read_csv("/content/finance_data.csv", batch_size=10000):
    print(df_chunk.shape)

# ðŸ’¤ Lazy Evaluation
pl.scan_csv("/content/finance_data.csv").filter(pl.col("feature_9") > 0).collect()

"""ðŸ”„ Category Encoding and Vector Ops"""

# ðŸ·ï¸ Category
df_csv.with_columns(pl.col("type").cast(pl.Categorical))

# âš¡ Vectorized Operations
df.with_columns((pl.col("col1") * 3).alias("tripled"))

"""# âœ… Summary of Polars Strengths
- **Speed:** Built in Rust, parallel execution by default.
- **Memory-efficient:** Lazy API uses minimal RAM.
- **Built-in Lazy Queries** for pipeline-style processing.
- **Excellent String & Date support**.
- **Arrow and Parquet integration** for efficient file operations.

ðŸ“Œ Use Polars when dealing with:
- Huge CSVs or structured data.
- Memory-sensitive environments.
- Data transformations with high performance requirements.

---
Now youâ€™re equipped to use Polars as a modern, high-performance DataFrame engine. Start replacing slow pandas pipelines with Polars for 10x performance boosts!

"""



"""
Act as a Python Expert who have worked on various advance concepts of Polars Libraries in Python with various Real life like use cases. now I am learning dataframe management and already learn pandas and in that I have seen polars lybraries alternative of of pandas with same use case time to time, so I though I should Learn polars for myself. Now as someone who never know anything about Polars. help me understand and learn best practices of polars library used on regular bases, and also help me learn how its useful while using in different use case like: -
DataFrame Creation: pl.DataFrame({"col1": [1, 2, 3], "col2": ["x", "y", "z"]})
Read CSV: pl.read_csv("file.csv")
Read Excel: pl.read_excel("file.xlsx")
Read Database: pl.read_database(query="SELECT * FROM table", connection=conn)
Write CSV: df.write_csv("output.csv")
Write Excel: df.write_excel("output.xlsx")
Write Database: df.write_database(table_name="table", connection=conn)
View First Rows: df.head(n=5)
View Last Rows: df.tail(n=5)
Schema Information: df.schema
Statistical Summary: df.describe()
Dimensions: df.height, df.width
List Columns: df.columns
Count Missing Values: df.null_count()
Drop Missing Values: df.drop_nulls()
Fill Missing Values: df.fill_null(value=0)
Select Single Column: df["column_name"]
Select Multiple Columns: df.select(["col1", "col2"])
Select Rows by Position: df.slice(start, length)
Filter Rows: df.filter(pl.col("col") > value)
String Contains Filter: df.filter(pl.col("col").str.contains("pattern"))
Drop Duplicates: df.unique() or df.unique(subset=["col1"])
Convert to Datetime: df.with_columns(pl.col("date").str.to_datetime())
Convert Data Type: df.with_columns(pl.col("col").cast(pl.Int32))
String Lowercase: df.with_columns(pl.col("col").str.to_lowercase())
String Strip: df.with_columns(pl.col("col").str.strip_chars())
String Replace: df.with_columns(pl.col("col").str.replace("old", "new"))
Rename Columns: df.rename({"old_name": "new_name"})
Create New Column: df.with_columns(pl.col("col") * 2.0.alias("new_col"))
Apply Function: df.with_columns(pl.col("col").map_elements(lambda x: func(x)))
GroupBy and Aggregate: df.group_by("col").agg(pl.col("value").mean().alias("mean_value"))
Join DataFrames: df.join(df2, on="key_column", how="inner")
Concatenate DataFrames: pl.concat([df1, df2], how="vertical")
Pivot Table: df.pivot(index="city", columns="department", values="salary", aggregate_function="mean")
Unpivot (Melt): df.unpivot(index="city", on=["salary", "bonus"])
Datetime Extraction: df.with_columns(pl.col("date").dt.year().alias("year"))
Dynamic GroupBy (Resampling): df.group_by_dynamic("date", every="1mo").agg(pl.col("value").sum())
Rolling Window: df.rolling(index_column="date", period="3d").agg(pl.col("value").mean())
Value Counts: df.group_by("col").len() or df["col"].value_counts()
Read in Chunks: pl.read_csv("file.csv", batch_size=10000)
Lazy Evaluation: pl.scan_csv("file.csv").filter(pl.col("age") > 25).collect()
Category Data Type: df.with_columns(pl.col("col").cast(pl.Categorical))
Vectorized Operations: pl.col("col") * 2 (used within with_columns or select), All I want to say is keep the examples diverse for different component seperate for all the components I wrote with polars, with polas's strength with its most used concept or practices of pydantic. Provide Cell by cell code and markdowns(appropriate to colab) I will copy paste it myself. Can Search on web for details.
"""

