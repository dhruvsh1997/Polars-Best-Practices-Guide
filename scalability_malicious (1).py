# -*- coding: utf-8 -*-
"""Scalability_Malicious.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14tWGisJPAt-G3Zb_TJ1_wsu__-AlfMxs
"""

from google.colab import drive
drive.mount('/content/drive')

!cd "/content/drive/MyDrive/Colab Files/MaximumBlockchainInput/Objective4-website monintoring usecase/3bc/"

#!/usr/bin/env python3
"""
ml_reputation_pipeline_full.py

Full end-to-end implementation:
1) Load final_miner_dataset.csv or synthesize
2) Feature extraction: consensus, transaction, network (graph)
3) Ensemble detectors: RandomForest (supervised if labels), IsolationForest (unsupervised), graph score
4) Bayesian stacking (Logistic meta-learner if labels available; else weighted fusion)
5) Reputation engine: Bayesian odds update + EWMA smoothing across epochs
6) Shard reconfiguration, reputation-based leader selection
7) Mitigation policies: WARN / QUARANTINE / SLASH
8) Save outputs and produce plots
"""

import os
import json
import math
import warnings
from collections import defaultdict

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split

warnings.filterwarnings("ignore")
np.random.seed(42)

###########################
# Configuration / params
###########################
INPUT_CSV = "final_miner_dataset.csv"   # your prepared dataset
OUT_RESULTS = "miner_detection_results.csv"
OUT_REP = "reputation.json"
NUM_SHARDS = 4
EWMA_ALPHA = 0.9       # smoothing for reputation updates
EPOCHS = 8             # number of epochs to simulate reputation updates
ISO_CONTAMINATION = 0.15
MITIGATE_WARN_T = 0.6
MITIGATE_QUARANTINE_T = 0.85
MITIGATE_SLASH_T = 0.95

###########################
# 1) Load or synthesize data
###########################
if os.path.exists(INPUT_CSV):
    df = pd.read_csv(INPUT_CSV)
    print(f"Loaded {INPUT_CSV} with shape {df.shape}")
else:
    # synthesize a dataset if not present (useful for quick demos)
    print(f"{INPUT_CSV} not found â€” creating synthetic dataset for demo.")
    miners = [f"miner_{i}" for i in range(1, 31)]
    rows = []
    for m in miners:
        blocks_mined = np.random.poisson(50)
        orphan_blocks = np.random.poisson(3)
        invalid_blocks = np.random.poisson(1)
        latency = np.random.uniform(0.4, 2.0)
        tx_submitted = np.random.randint(200, 2000)
        fraud_attempts = np.random.poisson(1)
        gas_price = np.random.randint(50_000_000, 2_000_000_000)
        value = np.random.exponential(1.0) * 100
        malicious_flag = 1 if np.random.rand() < 0.2 else 0
        rows.append([m, blocks_mined, orphan_blocks, invalid_blocks, latency,
                     tx_submitted, fraud_attempts, gas_price, value, malicious_flag])
    df = pd.DataFrame(rows, columns=[
        "miner_id", "blocks_mined", "orphan_blocks", "invalid_blocks", "latency",
        "tx_submitted", "fraud_attempts", "gas_price", "value", "label"
    ])
    df.to_csv(INPUT_CSV, index=False)
    print(f"Synthetic {INPUT_CSV} created with shape {df.shape}")

# Ensure expected columns exist
required_cols = ["miner_id", "blocks_mined", "orphan_blocks", "invalid_blocks",
                 "latency", "tx_submitted", "fraud_attempts", "gas_price", "value"]
for c in required_cols:
    if c not in df.columns:
        raise ValueError(f"Required column missing from dataset: {c}")

df = df.copy()
df.fillna(0, inplace=True)

# detect if labels exist for supervised training
has_labels = ("label" in df.columns and df["label"].nunique() > 1)
if has_labels:
    print("Ground-truth labels detected; supervised training will be used.")
else:
    print("No (useful) ground-truth labels found; supervised training will be skipped or pseudo-labeled.")

###########################
# 2) Feature extraction
###########################
total_blocks = df["blocks_mined"].sum() if df["blocks_mined"].sum() > 0 else 1
df["block_share"] = df["blocks_mined"] / total_blocks
df["orphan_rate"] = df["orphan_blocks"] / (df["blocks_mined"] + 1)
df["invalid_rate"] = df["invalid_blocks"] / (df["blocks_mined"] + 1)
df["txs_per_block"] = df["tx_submitted"] / (df["blocks_mined"] + 1)
df["fraud_rate"] = df["fraud_attempts"] / (df["tx_submitted"] + 1e-9)
df["avg_tx_value"] = df["value"] / (df["tx_submitted"] + 1e-9)

# Build or load transaction graph (if you have real TX edges, replace this block)
miners = df["miner_id"].tolist()
G = nx.DiGraph()
G.add_nodes_from(miners)
# create synthetic edges; weight relates to tx_submitted
malicious_set = set(df.loc[df.get("label", 0) == 1, "miner_id"].tolist()) if has_labels else set(
    np.random.choice(miners, size=max(1, int(0.2 * len(miners))), replace=False)
)
for m in miners:
    # number of outgoing neighbors
    out_deg = max(1, int(np.random.poisson(3)))
    targets = list(np.random.choice(miners, size=out_deg, replace=False))
    for t in targets:
        base = int(np.random.poisson(lam=max(1, df.loc[df["miner_id"] == m, "tx_submitted"].values[0] / 200)))
        weight = max(1, base + np.random.randint(0, 5))
        # boost weights if malicious to simulate collusion/spam
        if (m in malicious_set) or (t in malicious_set):
            weight = int(weight * (1.5 + np.random.rand()))
        G.add_edge(m, t, weight=weight)

in_deg = dict(G.in_degree(weight="weight"))
out_deg = dict(G.out_degree(weight="weight"))
pagerank = nx.pagerank(G, weight="weight")
undG = G.to_undirected()
clust = nx.clustering(undG, weight="weight")

df["in_deg"] = df["miner_id"].map(lambda x: in_deg.get(x, 0))
df["out_deg"] = df["miner_id"].map(lambda x: out_deg.get(x, 0))
df["pagerank"] = df["miner_id"].map(lambda x: pagerank.get(x, 0))
df["clustering"] = df["miner_id"].map(lambda x: clust.get(x, 0))

# Feature list used by ML detectors
feature_cols = [
    "blocks_mined", "block_share", "orphan_rate", "invalid_rate", "latency",
    "txs_per_block", "fraud_rate", "avg_tx_value", "in_deg", "out_deg", "pagerank", "clustering"
]

X = df[feature_cols].fillna(0).values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

###########################
# 3) Ensemble detectors
###########################
# Supervised detector (Random Forest) if labels exist; else create pseudo labels if desired.
if has_labels:
    y = df["label"].values
    rf = RandomForestClassifier(n_estimators=200, random_state=42)
    # small train-test split to check performance (we predict on full set after training)
    rf.fit(X_scaled, y)
    rf_prob = rf.predict_proba(X_scaled)[:, 1]
    print("RandomForest trained.")
else:
    rf = None
    rf_prob = np.zeros(len(df))
    # Optional: create pseudo-labels using IsolationForest on features then mark top anomalies as provisional positive
    # (not done here automatically)

# Unsupervised detector (IsolationForest)
iso = IsolationForest(contamination=ISO_CONTAMINATION, random_state=42)
iso.fit(X_scaled)
iso_scores = -iso.decision_function(X_scaled)  # higher = more anomalous
iso_norm = (iso_scores - iso_scores.min()) / (iso_scores.max() - iso_scores.min() + 1e-9)

# Graph-based detector: pagerank & degree anomalies
pagerank_z = np.abs((df["pagerank"] - df["pagerank"].mean()) / (df["pagerank"].std() + 1e-9))
deg = df["in_deg"] + df["out_deg"]
deg_z = np.abs((deg - deg.mean()) / (deg.std() + 1e-9))
graph_score_raw = (pagerank_z + deg_z) / 2.0
graph_score = (graph_score_raw - graph_score_raw.min()) / (graph_score_raw.max() - graph_score_raw.min() + 1e-9)

# attach detector outputs
df["rf_prob"] = rf_prob
df["iso_score"] = iso_norm
df["graph_score"] = graph_score

###########################
# 4) Bayesian stacking / probabilistic fusion
###########################
stack_X = pd.DataFrame({
    "rf_prob": rf_prob,
    "iso_score": iso_norm,
    "graph_score": graph_score
}, index=df.index)

# If labels available, fit a logistic meta-learner; else use conservative fixed weights
if has_labels:
    meta = LogisticRegression()
    meta.fit(stack_X.values, y)
    p_malicious = meta.predict_proba(stack_X.values)[:, 1]
    print("Trained meta-learner for probabilistic fusion. Coefs:", meta.coef_)
else:
    # default weights: favor unsupervised and graph detectors slightly when no labels
    w_rf, w_iso, w_graph = 0.3, 0.4, 0.3
    p_malicious = np.clip(w_rf * stack_X["rf_prob"] + w_iso * stack_X["iso_score"] + w_graph * stack_X["graph_score"], 1e-6, 1 - 1e-6)

df["p_malicious"] = p_malicious

###########################
# 5) Reputation engine (Bayesian odds update + EWMA smoothing)
###########################
# Initialize reputations (start at high trust)
reputation = {m: 0.95 for m in df["miner_id"].tolist()}
rep_history = {m: [] for m in reputation.keys()}
alpha = EWMA_ALPHA

for epoch in range(EPOCHS):
    # simulate observation noise (detection may vary per epoch)
    noise = np.random.normal(scale=0.02, size=len(df))
    p_obs = np.clip(df["p_malicious"].values + noise, 1e-6, 1 - 1e-6)
    for i, m in enumerate(df["miner_id"]):
        p = float(p_obs[i])
        prior_mal = 1.0 - reputation[m]
        odds_prior = (prior_mal + 1e-12) / (1.0 - prior_mal + 1e-12)
        odds_like = p / (1.0 - p)
        odds_post = odds_prior * odds_like
        posterior_mal = odds_post / (1.0 + odds_post)
        new_rep = 1.0 - posterior_mal
        # EWMA smoothing: keep reputations stable across noisy observations
        reputation[m] = float(np.clip(alpha * reputation[m] + (1 - alpha) * new_rep, 0.0, 1.0))
        rep_history[m].append(reputation[m])

df["final_reputation"] = df["miner_id"].map(reputation)

###########################
# 6) Shard reconfiguration & leader selection
###########################
miners_list = df["miner_id"].tolist()

# baseline: round-robin shard assignment
shard_map_baseline = {m: idx % NUM_SHARDS for idx, m in enumerate(miners_list)}

# reputation-driven assignment: sort by reputation and distribute to shards (round-robin on sorted list)
sorted_by_rep = sorted(miners_list, key=lambda x: reputation[x], reverse=True)
shard_map_rep = {m: idx % NUM_SHARDS for idx, m in enumerate(sorted_by_rep)}

def compute_shard_stats(shard_map):
    shard_avg = {}
    for s in range(NUM_SHARDS):
        mems = [m for m in miners_list if shard_map[m] == s]
        avg_r = np.mean([reputation[m] for m in mems]) if mems else 0.0
        shard_avg[s] = avg_r
    total_throughput = sum([(shard_avg[s] + 0.1) * 100.0 for s in shard_avg])  # arbitrary unit
    avg_latency = np.mean([1.0 / (shard_avg[s] + 0.1) for s in shard_avg])
    return shard_avg, total_throughput, avg_latency

shard_avg_base, tp_base, lat_base = compute_shard_stats(shard_map_baseline)
shard_avg_rep, tp_rep, lat_rep = compute_shard_stats(shard_map_rep)

###########################
# 7) Mitigation policies (action assignment)
###########################
actions = {}
for i, m in enumerate(df["miner_id"]):
    p = float(df.loc[i, "p_malicious"])
    rep = reputation[m]
    if p >= MITIGATE_SLASH_T or rep < 0.05:
        actions[m] = "SLASH"
    elif p >= MITIGATE_QUARANTINE_T or rep < 0.2:
        actions[m] = "QUARANTINE"
    elif p >= MITIGATE_WARN_T:
        actions[m] = "WARN"
    else:
        actions[m] = "ALLOW"

df["action"] = df["miner_id"].map(actions)

# Apply slashing effect: penalize reputation for slashed miners
slashed = [m for m, a in actions.items() if a == "SLASH"]
for m in slashed:
    reputation[m] = max(0.0, reputation[m] - 0.5)
    df.loc[df["miner_id"] == m, "final_reputation"] = reputation[m]

###########################
# 8) Evaluation & save outputs
###########################
# detection metrics (if labels exist)
if has_labels:
    y_true = df["label"].values
    y_pred = (df["p_malicious"] >= 0.5).astype(int)
    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary", zero_division=0)
    acc = accuracy_score(y_true, y_pred)
    roc = roc_auc_score(y_true, df["p_malicious"].values)
else:
    prec = rec = f1 = acc = roc = float("nan")

results_summary = {
    "num_miners": len(miners_list),
    "tp_baseline": tp_base,
    "lat_base": lat_base,
    "tp_rep_driven": tp_rep,
    "lat_rep_driven": lat_rep,
    "detection_precision": prec,
    "detection_recall": rec,
    "detection_f1": f1,
    "detection_acc": acc,
    "detection_auc": roc,
    "num_warn": sum(1 for v in actions.values() if v == "WARN"),
    "num_quarantine": sum(1 for v in actions.values() if v == "QUARANTINE"),
    "num_slash": len(slashed)
}

print("\n=== Summary Results ===")
for k, v in results_summary.items():
    print(f"{k}: {v}")

# Save outputs
df.to_csv(OUT_RESULTS, index=False)
with open(OUT_REP, "w") as f:
    json.dump(reputation, f, indent=2)

print(f"\nSaved outputs: {OUT_RESULTS}, {OUT_REP}")

###########################
# 9) Plots: reputation distribution, shard comparison, flagged list
###########################
# Reputation distribution
plt.figure(figsize=(10, 4))
rep_sorted = sorted([reputation[m] for m in miners_list], reverse=True)
plt.bar(range(len(rep_sorted)), rep_sorted, color="orange")
plt.title("Final reputation distribution (sorted)")
plt.xlabel("Miner rank")
plt.ylabel("Reputation")
plt.tight_layout()
plt.show()

# Shard avg rep comparison
labels = [f"shard_{i}" for i in range(NUM_SHARDS)]
base_vals = [shard_avg_base[i] for i in range(NUM_SHARDS)]
rep_vals = [shard_avg_rep[i] for i in range(NUM_SHARDS)]
x = np.arange(len(labels))
plt.figure(figsize=(8, 4))
plt.bar(x - 0.18, base_vals, width=0.36, label="baseline")
plt.bar(x + 0.18, rep_vals, width=0.36, label="rep-driven")
plt.xticks(x, labels)
plt.ylabel("Average reputation")
plt.title("Shard average reputation: baseline vs reputation-driven")
plt.legend()
plt.tight_layout()
plt.show()

# Flagged miners list
flagged = df[df["action"] != "ALLOW"][["miner_id", "p_malicious", "final_reputation", "action"]].sort_values("p_malicious", ascending=False)
if not flagged.empty:
    print("\nFlagged miners (WARN/QUARANTINE/SLASH):")
    print(flagged.to_string(index=False))
else:
    print("\nNo flagged miners in this run (thresholds may be conservative).")

# End of script

#!/usr/bin/env python3
"""
simulator_batch_final_sharded_fixed.py

Final Blockchain Simulator with:
 - Proof of Work (PoW)
 - Proof of Stake (PoS)
 - ML + Reputation + Sharding (Proposed System)

Fixes:
âœ… Numeric-only feature preprocessing (avoids 'could not convert string to float')
âœ… Dynamic sharding with reputation-based reconfiguration
âœ… Robust ML-based malicious detection
âœ… Parallel shard-level throughput and latency tracking
"""

import os, json, random, math
import numpy as np
import pandas as pd
import networkx as nx
from tqdm import trange
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# -------------------- CONFIG --------------------
OUTDIR = "outputs_sharded_fixed"
MODES = ["pow", "pos", "ml_reputation"]
NUM_MINERS = 30
BLOCKS_PER_RUN = 800
EPOCH_BLOCKS = 80
EVAL_MALICIOUS_FRAC = [0.0, 0.1, 0.2, 0.3]
SEEDS = [0, 1, 2, 3]
NUM_SHARDS = 3
ML_DETECTION = True
ALPHA = 0.9  # EWMA factor
SLASH_PENALTY = 0.4

# -------------------- HELPERS --------------------
def ensure_outdir(path):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def gini(arr):
    a = np.sort(np.array(arr))
    n = len(a)
    if np.sum(a) == 0:
        return 0
    cum = np.cumsum(a)
    return (n + 1 - 2 * np.sum(cum) / cum[-1]) / n

def nakamoto_coefficient(counts):
    total = np.sum(counts)
    shares = np.sort(counts / total)[::-1]
    cum = np.cumsum(shares)
    return int(np.argmax(cum >= 0.51) + 1)

# -------------------- LOAD YOUR DATASET --------------------
print("ðŸ“‚ Loading final_miner_dataset.csv...")
miner_df = pd.read_csv("final_miner_dataset.csv")
miner_df.fillna(0, inplace=True)

# Filter numeric columns only
numeric_cols = [
    c for c in miner_df.columns
    if c not in ["miner_id", "label", "malicious_flag"]
    and np.issubdtype(miner_df[c].dtype, np.number)
]
feature_cols = numeric_cols

label_col = "malicious_flag" if "malicious_flag" in miner_df.columns else "label"

# Convert to numeric floats
X_real = miner_df[numeric_cols].apply(pd.to_numeric, errors="coerce").fillna(0).astype(float)
y_real = miner_df[label_col].astype(int)

# Scale
scaler = StandardScaler()
X_real_scaled = scaler.fit_transform(X_real)

# Train ML models
rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
rf_model.fit(X_real_scaled, y_real)
iso_model = IsolationForest(contamination=0.15, random_state=42)
iso_model.fit(X_real_scaled)

rf_probs = rf_model.predict_proba(X_real_scaled)[:, 1]
iso_scores = -iso_model.decision_function(X_real_scaled)
iso_norm = (iso_scores - iso_scores.min()) / (iso_scores.max() - iso_scores.min() + 1e-9)
stack_X = np.vstack([rf_probs, iso_norm, np.random.rand(len(rf_probs))]).T
meta_model = LogisticRegression()
meta_model.fit(stack_X, y_real)

print(f"âœ… ML models trained on {len(miner_df)} samples | Malicious nodes: {sum(y_real)}")

# -------------------- CORE SIMULATION --------------------
def simulate_run(mode="ml_reputation", seed=0, malicious_frac=0.2):
    np.random.seed(seed)
    random.seed(seed)

    miners = [f"miner_{i}" for i in range(NUM_MINERS)]
    num_mal = max(1, int(NUM_MINERS * malicious_frac))
    malicious_miners = set(np.random.choice(miners, num_mal, replace=False))
    reputation = {m: 0.95 for m in miners}
    produced_counts = {m: 0 for m in miners}
    blocks_log = []

    # -------------------- Shard Initialization --------------------
    miners_per_shard = NUM_MINERS // NUM_SHARDS
    shards = {i: [] for i in range(NUM_SHARDS)}
    for i, m in enumerate(miners):
        shards[i % NUM_SHARDS].append(m)
    shard_throughput = {i: [] for i in range(NUM_SHARDS)}
    shard_latency = {i: [] for i in range(NUM_SHARDS)}
    shard_leaders = {i: shards[i][0] for i in shards}

    # -------------------- Block Generation --------------------
    current_time = 0.0
    for b in range(BLOCKS_PER_RUN):
        # Miner selection
        if mode == "pow":
            probs = np.random.dirichlet(np.ones(NUM_MINERS))
        elif mode == "pos":
            probs = np.random.dirichlet(np.ones(NUM_MINERS) * 1.5)
        elif mode == "ml_reputation":
            probs = np.array([reputation[m] for m in miners])
            probs = probs / probs.sum()
        else:
            raise ValueError("Unknown mode")

        producer = np.random.choice(miners, p=probs)
        #tx_count = np.random.poisson(20)
        #latency = np.random.uniform(0.05, 0.2)
        # -------------------- Scalable Block Simulation --------------------
        # Adaptive transaction volume and latency for realistic throughput
        base_tx = np.random.poisson(1500)  # average transactions per block
        shard_factor = NUM_SHARDS  # number of parallel shards
        # Add small variability to simulate load imbalance across shards
        tx_count = max(500, base_tx + np.random.randint(-300, 300)) * shard_factor

        # Simulated block latency â€” lower for higher shard counts
        # Sharding parallelizes validation, reducing total delay
        latency = np.random.uniform(0.8, 1.5) / max(1, shard_factor / 2)

        # Append block data
        blocks_log.append({
            "block_id": b + 1,
            "timestamp": current_time,
            "producer": producer,
            "tx_count": tx_count,
            "latency": latency
        })
        produced_counts[producer] += 1
        current_time += latency + np.random.exponential(0.5)

        # -------------------- Record per-shard metrics --------------------
        shard_id = next((sid for sid, mlist in shards.items() if producer in mlist), None)
        if shard_id is not None:
            shard_throughput[shard_id].append(tx_count)
            shard_latency[shard_id].append(latency)

        # -------------------- ML Detection + Shard Reconfiguration --------------------
        if mode == "ml_reputation" and ML_DETECTION and (b + 1) % EPOCH_BLOCKS == 0:
            df = pd.DataFrame(blocks_log)
            feat = df.groupby("producer").agg({
                "tx_count": "mean", "latency": "mean"
            }).reset_index().rename(columns={"producer": "miner_id"})
            feat["blocks_mined"] = df["producer"].value_counts().reindex(feat["miner_id"]).values
            feat.fillna(0, inplace=True)

            # Extract numeric features safely
            #feat_numeric = feat.select_dtypes(include=[np.number])
            #valid_cols = [c for c in feature_cols if c in feat_numeric.columns]
            #X_epoch = feat_numeric[valid_cols].apply(pd.to_numeric, errors="coerce").fillna(0).astype(float).values
            #X_epoch = scaler.transform(X_epoch)

            # Align simulated features with training features
            feat_numeric = feat.select_dtypes(include=[np.number]).copy()

            # Add any missing columns (fill with zeros)
            for col in feature_cols:
                if col not in feat_numeric.columns:
                    feat_numeric[col] = 0.0

            # Ensure column order matches training data
            feat_numeric = feat_numeric[feature_cols]

            # Convert all to float and handle NaNs
            X_epoch = feat_numeric.apply(pd.to_numeric, errors="coerce").fillna(0).astype(float).values

            # Transform using scaler trained on full dataset
            X_epoch = scaler.transform(X_epoch)

            rf_prob = rf_model.predict_proba(X_epoch)[:, 1]
            iso_score = -iso_model.decision_function(X_epoch)
            iso_norm = (iso_score - iso_score.min()) / (iso_score.max() - iso_score.min() + 1e-9)
            graph_norm = np.random.rand(len(feat))
            stack_X = np.vstack([rf_prob, iso_norm, graph_norm]).T
            p_mal = meta_model.predict_proba(stack_X)[:, 1]
            feat["p_malicious"] = p_mal

            # Update reputations
            for _, row in feat.iterrows():
                m = row["miner_id"]
                if m not in reputation:
                    continue
                p = row["p_malicious"]
                prior_mal = 1.0 - reputation[m]
                odds_prior = (prior_mal + 1e-9) / (1.0 - prior_mal + 1e-9)
                odds_like = p / (1.0 - p + 1e-9)
                odds_post = odds_prior * odds_like
                posterior_mal = odds_post / (1.0 + odds_post)
                new_rep = 1.0 - posterior_mal
                reputation[m] = ALPHA * reputation[m] + (1 - ALPHA) * new_rep

                # Mitigation
                if p >= 0.95:
                    reputation[m] = max(0.0, reputation[m] - SLASH_PENALTY)
                elif p >= 0.85:
                    reputation[m] = max(0.1, reputation[m] - 0.2)
                elif p >= 0.6:
                    reputation[m] = max(0.3, reputation[m] - 0.1)

            # -------------------- Shard Reconfiguration --------------------
            print(f"â™»ï¸ Epoch {(b+1)//EPOCH_BLOCKS}: Reconfiguring shards...")
            sorted_miners = sorted(reputation.items(), key=lambda x: x[1], reverse=True)
            miners_sorted = [m for m, _ in sorted_miners]
            for i in range(NUM_SHARDS):
                shards[i] = miners_sorted[i::NUM_SHARDS]
            shard_leaders = {sid: max(shards[sid], key=lambda m: reputation[m]) for sid in shards}
            print("â­ New shard leaders:", shard_leaders)

    # -------------------- Compute Metrics --------------------
    total_time = blocks_log[-1]["timestamp"] - blocks_log[0]["timestamp"]
    total_tx = sum([b["tx_count"] for b in blocks_log])
    tps = total_tx / total_time
    avg_latency = np.mean([b["latency"] for b in blocks_log])
    gini_val = gini(list(produced_counts.values()))
    decentralization = 1 - gini_val
    nakamoto = nakamoto_coefficient(np.array(list(produced_counts.values())))

    # Shard-level metrics
    shard_perf = []
    for sid in shards:
        tx_sum = np.sum(shard_throughput[sid]) if shard_throughput[sid] else 0
        lat_avg = np.mean(shard_latency[sid]) if shard_latency[sid] else 0
        shard_perf.append({
            "shard_id": sid,
            "throughput": tx_sum / max(1, total_time),
            "avg_latency": lat_avg,
            "leader": shard_leaders[sid]
        })

    meta = {
        "mode": mode,
        "seed": seed,
        "malicious_frac": malicious_frac,
        "tps": float(tps),
        "latency": float(avg_latency),
        "decentralization": float(decentralization),
        "nakamoto": int(nakamoto),
        "avg_reputation": np.mean(list(reputation.values())),
        "avg_shard_tps": np.mean([s["throughput"] for s in shard_perf]),
        "avg_shard_latency": np.mean([s["avg_latency"] for s in shard_perf]),
        "num_shards": NUM_SHARDS,
        "shard_performance": shard_perf
    }
    return meta

# -------------------- RUN BATCH --------------------
def run_batch():
    ensure_outdir(OUTDIR)
    results = []
    for mode in MODES:
        print(f"\n===== Running mode: {mode.upper()} =====")
        for frac in EVAL_MALICIOUS_FRAC:
            for seed in SEEDS:
                meta = simulate_run(mode=mode, seed=seed, malicious_frac=frac)
                results.append(meta)
                print(f"[{mode}] mal={frac:.2f} seed={seed} -> TPS={meta['tps']:.2f}, Lat={meta['latency']:.3f}, Dec={meta['decentralization']:.3f}")
        pd.DataFrame(results).to_csv(os.path.join(OUTDIR, f"{mode}_summary.csv"), index=False)
    pd.DataFrame(results).to_csv(os.path.join(OUTDIR, "all_results_summary.csv"), index=False)
    print("\nâœ… Simulation completed! Results saved in:", OUTDIR)

if __name__ == "__main__":
    run_batch()

#!/usr/bin/env python3
"""
simulator_batch_final_sharded_plot.py

Final Blockchain Simulator with:
âœ… PoW, PoS, ML + Reputation + Sharding
âœ… ML ensemble (RF + IF + Bayesian stacking)
âœ… Reputation + EWMA + dynamic sharding
âœ… Automatic plotting: throughput, latency, decentralization, shard stats
"""

import os, json, random, math
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from tqdm import trange
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# -------------------- CONFIG --------------------
OUTDIR = "outputs_sharded_plot"
MODES = ["pow", "pos", "ml_reputation"]
NUM_MINERS = 30
BLOCKS_PER_RUN = 800
EPOCH_BLOCKS = 80
EVAL_MALICIOUS_FRAC = [0.0, 0.1, 0.2, 0.3]
SEEDS = [0, 1, 2, 3]
NUM_SHARDS = 3
ML_DETECTION = True
ALPHA = 0.9
SLASH_PENALTY = 0.4

# -------------------- HELPERS --------------------
def ensure_outdir(path):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def gini(arr):
    a = np.sort(np.array(arr))
    n = len(a)
    if np.sum(a) == 0: return 0
    cum = np.cumsum(a)
    return (n + 1 - 2 * np.sum(cum) / cum[-1]) / n

def nakamoto_coefficient(counts):
    total = np.sum(counts)
    shares = np.sort(counts / total)[::-1]
    cum = np.cumsum(shares)
    return int(np.argmax(cum >= 0.51) + 1)

# -------------------- LOAD DATASET --------------------
print("ðŸ“‚ Loading final_miner_dataset.csv...")
miner_df = pd

#!/usr/bin/env python3
"""
simulator_batch_final_sharded_plot.py

Final Blockchain Simulator with:
âœ… PoW, PoS, ML + Reputation + Sharding
âœ… ML ensemble (RF + IF + Bayesian stacking)
âœ… Reputation + EWMA + dynamic sharding
âœ… Automatic plotting: throughput, latency, decentralization, shard stats
"""

import os, json, random, math
import numpy as np
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from tqdm import trange
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# -------------------- CONFIG --------------------
OUTDIR = "outputs_sharded_plot"
MODES = ["pow", "pos", "ml_reputation"]
NUM_MINERS = 30
BLOCKS_PER_RUN = 800
EPOCH_BLOCKS = 80
EVAL_MALICIOUS_FRAC = [0.0, 0.1, 0.2, 0.3]
SEEDS = [0, 1, 2, 3]
NUM_SHARDS = 3
ML_DETECTION = True
ALPHA = 0.9
SLASH_PENALTY = 0.4

# -------------------- HELPERS --------------------
def ensure_outdir(path):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def gini(arr):
    a = np.sort(np.array(arr))
    n = len(a)
    if np.sum(a) == 0: return 0
    cum = np.cumsum(a)
    return (n + 1 - 2 * np.sum(cum) / cum[-1]) / n

def nakamoto_coefficient(counts):
    total = np.sum(counts)
    shares = np.sort(counts / total)[::-1]
    cum = np.cumsum(shares)
    return int(np.argmax(cum >= 0.51) + 1)

# -------------------- LOAD DATASET --------------------
print("ðŸ“‚ Loading final_miner_dataset.csv...")
miner_df = pd.read_csv("final_miner_dataset.csv")
miner_df.fillna(0, inplace=True)

# Keep only numeric features
numeric_cols = [c for c in miner_df.columns
                if c not in ["miner_id", "label", "malicious_flag"]
                and np.issubdtype(miner_df[c].dtype, np.number)]
feature_cols = numeric_cols
label_col = "malicious_flag" if "malicious_flag" in miner_df.columns else "label"

X_real = miner_df[numeric_cols].apply(pd.to_numeric, errors="coerce").fillna(0).astype(float)
y_real = miner_df[label_col].astype(int)

scaler = StandardScaler()
X_real_scaled = scaler.fit_transform(X_real)

# Train models
rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
rf_model.fit(X_real_scaled, y_real)
iso_model = IsolationForest(contamination=0.15, random_state=42)
iso_model.fit(X_real_scaled)

rf_probs = rf_model.predict_proba(X_real_scaled)[:, 1]
iso_scores = -iso_model.decision_function(X_real_scaled)
iso_norm = (iso_scores - iso_scores.min()) / (iso_scores.max() - iso_scores.min() + 1e-9)
stack_X = np.vstack([rf_probs, iso_norm, np.random.rand(len(rf_probs))]).T
meta_model = LogisticRegression()
meta_model.fit(stack_X, y_real)

print(f"âœ… ML models trained on {len(miner_df)} samples | Malicious nodes: {sum(y_real)}")

# -------------------- SIMULATION FUNCTION --------------------
def simulate_run(mode="ml_reputation", seed=0, malicious_frac=0.2):
    np.random.seed(seed)
    random.seed(seed)

    miners = [f"miner_{i}" for i in range(NUM_MINERS)]
    num_mal = max(1, int(NUM_MINERS * malicious_frac))
    malicious_miners = set(np.random.choice(miners, num_mal, replace=False))
    reputation = {m: 0.95 for m in miners}
    produced_counts = {m: 0 for m in miners}
    blocks_log = []

    # Shard initialization
    miners_per_shard = NUM_MINERS // NUM_SHARDS
    shards = {i: [] for i in range(NUM_SHARDS)}
    for i, m in enumerate(miners):
        shards[i % NUM_SHARDS].append(m)
    shard_throughput = {i: [] for i in range(NUM_SHARDS)}
    shard_latency = {i: [] for i in range(NUM_SHARDS)}
    shard_leaders = {i: shards[i][0] for i in shards}

    # Block generation
    current_time = 0.0
    for b in range(BLOCKS_PER_RUN):
        if mode == "pow":
            probs = np.random.dirichlet(np.ones(NUM_MINERS))
        elif mode == "pos":
            probs = np.random.dirichlet(np.ones(NUM_MINERS) * 1.5)
        elif mode == "ml_reputation":
            probs = np.array([reputation[m] for m in miners])
            probs = probs / probs.sum()
        else:
            raise ValueError("Unknown mode")

        producer = np.random.choice(miners, p=probs)
        #tx_count = np.random.poisson(20)
        #latency = np.random.uniform(0.05, 0.2)
          # -------------------- Scalable Block Simulation --------------------
        # Adaptive transaction volume and latency for realistic throughput
        base_tx = np.random.poisson(1500)  # average transactions per block
        shard_factor = NUM_SHARDS  # number of parallel shards
        # Add small variability to simulate load imbalance across shards
        tx_count = max(500, base_tx + np.random.randint(-300, 300)) * shard_factor

        # Simulated block latency â€” lower for higher shard counts
        # Sharding parallelizes validation, reducing total delay
        latency = np.random.uniform(0.8, 1.5) / max(1, shard_factor / 2)

        # Append block data
        blocks_log.append({
            "block_id": b + 1,
            "timestamp": current_time,
            "producer": producer,
            "tx_count": tx_count,
            "latency": latency
        })
        produced_counts[producer] += 1
        current_time += latency + np.random.exponential(0.5)

        # Per-shard stats
        shard_id = next((sid for sid, mlist in shards.items() if producer in mlist), None)
        if shard_id is not None:
            shard_throughput[shard_id].append(tx_count)
            shard_latency[shard_id].append(latency)

        # ML Detection + Shard reconfiguration
        if mode == "ml_reputation" and ML_DETECTION and (b + 1) % EPOCH_BLOCKS == 0:
            df = pd.DataFrame(blocks_log)
            feat = df.groupby("producer").agg({
                "tx_count": "mean", "latency": "mean"
            }).reset_index().rename(columns={"producer": "miner_id"})
            feat["blocks_mined"] = df["producer"].value_counts().reindex(feat["miner_id"]).values
            feat.fillna(0, inplace=True)

            # Align feature shape
            feat_numeric = feat.select_dtypes(include=[np.number]).copy()
            for col in feature_cols:
                if col not in feat_numeric.columns:
                    feat_numeric[col] = 0.0
            feat_numeric = feat_numeric[feature_cols]
            X_epoch = feat_numeric.apply(pd.to_numeric, errors="coerce").fillna(0).astype(float).values
            X_epoch = scaler.transform(X_epoch)

            # Predict malicious probabilities
            rf_prob = rf_model.predict_proba(X_epoch)[:, 1]
            iso_score = -iso_model.decision_function(X_epoch)
            iso_norm = (iso_score - iso_score.min()) / (iso_score.max() - iso_score.min() + 1e-9)
            graph_norm = np.random.rand(len(feat))
            stack_X = np.vstack([rf_prob, iso_norm, graph_norm]).T
            p_mal = meta_model.predict_proba(stack_X)[:, 1]
            feat["p_malicious"] = p_mal

            # Reputation update
            for _, row in feat.iterrows():
                m = row["miner_id"]
                if m not in reputation: continue
                p = row["p_malicious"]
                prior_mal = 1.0 - reputation[m]
                odds_prior = (prior_mal + 1e-9) / (1.0 - prior_mal + 1e-9)
                odds_like = p / (1.0 - p + 1e-9)
                odds_post = odds_prior * odds_like
                posterior_mal = odds_post / (1.0 + odds_post)
                new_rep = 1.0 - posterior_mal
                reputation[m] = ALPHA * reputation[m] + (1 - ALPHA) * new_rep
                if p >= 0.95:
                    reputation[m] = max(0.0, reputation[m] - SLASH_PENALTY)
                elif p >= 0.85:
                    reputation[m] = max(0.1, reputation[m] - 0.2)
                elif p >= 0.6:
                    reputation[m] = max(0.3, reputation[m] - 0.1)

            # Shard reconfiguration
            sorted_miners = sorted(reputation.items(), key=lambda x: x[1], reverse=True)
            miners_sorted = [m for m, _ in sorted_miners]
            for i in range(NUM_SHARDS):
                shards[i] = miners_sorted[i::NUM_SHARDS]
            shard_leaders = {sid: max(shards[sid], key=lambda m: reputation[m]) for sid in shards}
            print(f"â™»ï¸ Epoch {(b+1)//EPOCH_BLOCKS}: Reconfiguring shards, leaders:", shard_leaders)

    # Compute metrics
    total_time = blocks_log[-1]["timestamp"] - blocks_log[0]["timestamp"]
    total_tx = sum([b["tx_count"] for b in blocks_log])
    tps = total_tx / total_time
    avg_latency = np.mean([b["latency"] for b in blocks_log])
    gini_val = gini(list(produced_counts.values()))
    decentralization = 1 - gini_val
    nakamoto = nakamoto_coefficient(np.array(list(produced_counts.values())))

    shard_perf = []
    for sid in shards:
        tx_sum = np.sum(shard_throughput[sid]) if shard_throughput[sid] else 0
        lat_avg = np.mean(shard_latency[sid]) if shard_latency[sid] else 0
        shard_perf.append({
            "shard_id": sid,
            "throughput": tx_sum / max(1, total_time),
            "avg_latency": lat_avg,
            "leader": shard_leaders[sid]
        })

    return {
        "mode": mode, "seed": seed, "malicious_frac": malicious_frac,
        "tps": float(tps), "latency": float(avg_latency),
        "decentralization": float(decentralization), "nakamoto": int(nakamoto),
        "avg_shard_tps": np.mean([s["throughput"] for s in shard_perf]),
        "avg_shard_latency": np.mean([s["avg_latency"] for s in shard_perf])
    }

# -------------------- RUN + PLOT --------------------
def run_batch_and_plot():
    ensure_outdir(OUTDIR)
    results = []
    for mode in MODES:
        print(f"\n===== Running mode: {mode.upper()} =====")
        for frac in EVAL_MALICIOUS_FRAC:
            for seed in SEEDS:
                meta = simulate_run(mode=mode, seed=seed, malicious_frac=frac)
                results.append(meta)
                print(f"[{mode}] mal={frac:.2f} seed={seed} -> TPS={meta['tps']:.2f}, Lat={meta['latency']:.3f}")
    df = pd.DataFrame(results)
    df.to_csv(os.path.join(OUTDIR, "all_results_summary.csv"), index=False)
    print("\nâœ… Simulation complete â€” generating plots...")

    # === Plot 1: Throughput vs Malicious Fraction ===
    plt.figure(figsize=(8,6))
    for mode in MODES:
        subset = df[df["mode"] == mode].groupby("malicious_frac")["tps"].mean()
        plt.plot(subset.index, subset.values, marker="o", label=mode.upper())
    plt.xlabel("Malicious Fraction")
    plt.ylabel("Throughput (TPS)")
    plt.title("Throughput vs Malicious Node Fraction")
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "throughput_vs_malicious.png"))
    plt.close()

    # === Plot 2: Latency vs Malicious Fraction ===
    plt.figure(figsize=(8,6))
    for mode in MODES:
        subset = df[df["mode"] == mode].groupby("malicious_frac")["latency"].mean()
        plt.plot(subset.index, subset.values, marker="s", label=mode.upper())
    plt.xlabel("Malicious Fraction")
    plt.ylabel("Latency (s)")
    plt.title("Latency vs Malicious Node Fraction")
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "latency_vs_malicious.png"))
    plt.close()

    # === Plot 3: Decentralization vs Malicious Fraction ===
    plt.figure(figsize=(8,6))
    for mode in MODES:
        subset = df[df["mode"] == mode].groupby("malicious_frac")["decentralization"].mean()
        plt.plot(subset.index, subset.values, marker="^", label=mode.upper())
    plt.xlabel("Malicious Fraction")
    plt.ylabel("Decentralization (1 - Gini)")
    plt.title("Decentralization vs Malicious Node Fraction")
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "decentralization_vs_malicious.png"))
    plt.close()

    # === Plot 4: Average Shard TPS and Latency ===
    plt.figure(figsize=(8,6))
    ml_df = df[df["mode"] == "ml_reputation"]
    plt.bar(ml_df["malicious_frac"], ml_df["avg_shard_tps"], color="teal", alpha=0.7, label="Shard TPS")
    plt.plot(ml_df["malicious_frac"], ml_df["avg_shard_latency"], color="orange", marker="o", label="Shard Latency (s)")
    plt.xlabel("Malicious Fraction")
    plt.ylabel("TPS / Latency")
    plt.title("Shard-Level Performance (ML + Reputation)")
    plt.legend(); plt.grid(True); plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "shard_performance.png"))
    plt.close()

    # === Plot 6: Scalability â€” Throughput vs Number of Shards ===
    print("ðŸ“Š Measuring scalability: Throughput vs Number of Shards...")

    shard_scaling_results = []
    test_shard_counts = [1, 2, 3, 5, 10]

    for shard_count in test_shard_counts:
        global NUM_SHARDS
        NUM_SHARDS = shard_count
        meta = simulate_run(mode="ml_reputation", seed=0, malicious_frac=0.1)
        shard_scaling_results.append({
            "num_shards": shard_count,
            "tps": meta["tps"],
            "latency": meta["latency"]
        })
        print(f"ðŸ§© Shards={shard_count} -> TPS={meta['tps']:.2f}, Lat={meta['latency']:.3f}")

    scale_df = pd.DataFrame(shard_scaling_results)
    scale_df.to_csv(os.path.join(OUTDIR, "scalability_vs_shards.csv"), index=False)

    # --- Plot ---
    plt.figure(figsize=(8,6))
    plt.plot(scale_df["num_shards"], scale_df["tps"], marker="o", color="teal", linewidth=2, label="Throughput (TPS)")
    plt.plot(scale_df["num_shards"], scale_df["latency"] * 100, marker="s", color="orange", linewidth=2, label="Latency (x100 for scale)")
    plt.title("Scalability: Throughput vs Number of Shards", fontsize=14, fontweight="bold")
    plt.xlabel("Number of Shards")
    plt.ylabel("Throughput (TPS)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "throughput_vs_shards.png"), dpi=300)
    plt.close()
    print("âœ… Scalability plot saved: throughput_vs_shards.png")

     # === Plot 7: 2D Heatmap â€” Throughput vs Shards vs Malicious Fraction ===
    print("ðŸ“Š Generating 2D scalability-resilience heatmap...")

    shard_counts = [1, 2, 3, 5, 10]
    malicious_levels = [0.0, 0.1, 0.2, 0.3]
    heatmap_data = []

    for shard_count in shard_counts:
        #global NUM_SHARDS
        NUM_SHARDS = shard_count
        for mal_frac in malicious_levels:
            meta = simulate_run(mode="ml_reputation", seed=1, malicious_frac=mal_frac)
            heatmap_data.append({
                "num_shards": shard_count,
                "malicious_frac": mal_frac,
                "tps": meta["tps"],
                "latency": meta["latency"]
            })
            print(f"ðŸ§© Shards={shard_count}, Mal={mal_frac:.1f} -> TPS={meta['tps']:.2f}, Lat={meta['latency']:.3f}")

    heatmap_df = pd.DataFrame(heatmap_data)
    heatmap_df.to_csv(os.path.join(OUTDIR, "scalability_heatmap_data.csv"), index=False)

    # Pivot for heatmap plotting
    #pivot_tps = heatmap_df.pivot("num_shards", "malicious_frac", "tps")
    pivot_tps = heatmap_df.pivot(index="num_shards", columns="malicious_frac", values="tps")

    plt.figure(figsize=(8,6))
    im = plt.imshow(pivot_tps, cmap="viridis", aspect="auto", origin="lower")
    plt.colorbar(im, label="Throughput (TPS)")
    plt.title("Throughput vs Shards vs Malicious Fraction", fontsize=14, fontweight="bold")
    plt.xlabel("Malicious Node Fraction")
    plt.ylabel("Number of Shards")
    plt.xticks(ticks=np.arange(len(malicious_levels)), labels=[f"{m:.1f}" for m in malicious_levels])
    plt.yticks(ticks=np.arange(len(shard_counts)), labels=[str(s) for s in shard_counts])
    plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "throughput_heatmap.png"), dpi=300)
    plt.close()
    print("âœ… 2D Heatmap saved: throughput_heatmap.png")

      # === Plot 7: 2D Heatmap with Latency Contours ===
    print("ðŸ“Š Generating enhanced 2D throughputâ€“latency heatmap...")

    shard_counts = [1, 2, 3, 5, 10]
    malicious_levels = [0.0, 0.1, 0.2, 0.3]
    heatmap_data = []

    for shard_count in shard_counts:
        #global NUM_SHARDS
        NUM_SHARDS = shard_count
        for mal_frac in malicious_levels:
            meta = simulate_run(mode="ml_reputation", seed=1, malicious_frac=mal_frac)
            heatmap_data.append({
                "num_shards": shard_count,
                "malicious_frac": mal_frac,
                "tps": meta["tps"],
                "latency": meta["latency"]
            })
            print(f"ðŸ§© Shards={shard_count}, Mal={mal_frac:.1f} -> TPS={meta['tps']:.2f}, Lat={meta['latency']:.3f}")

    heatmap_df = pd.DataFrame(heatmap_data)
    heatmap_df.to_csv(os.path.join(OUTDIR, "scalability_heatmap_data.csv"), index=False)

    # Pivot data
    pivot_tps = heatmap_df.pivot(index="num_shards", columns="malicious_frac", values="tps")
    pivot_lat = heatmap_df.pivot(index="num_shards", columns="malicious_frac", values="latency")

    # --- Plot with latency contours ---
    plt.figure(figsize=(8,6))
    im = plt.imshow(pivot_tps, cmap="viridis", aspect="auto", origin="lower")
    cbar = plt.colorbar(im)
    cbar.set_label("Throughput (TPS)", fontsize=11)

    # Add contour lines for latency (scaled if needed)
    X, Y = np.meshgrid(pivot_tps.columns, pivot_tps.index)
    CS = plt.contour(X, Y, pivot_lat.values, colors="white", linewidths=1.2)
    plt.clabel(CS, inline=True, fontsize=9, fmt="%.2f")

    # Labels and styling
    plt.title("Pareto Efficiency: Throughput (color) vs Latency (contours)", fontsize=14, fontweight="bold")
    plt.xlabel("Malicious Node Fraction", fontsize=12)
    plt.ylabel("Number of Shards", fontsize=12)
    plt.xticks(ticks=np.arange(len(pivot_tps.columns)), labels=[f"{m:.1f}" for m in pivot_tps.columns])
    plt.yticks(ticks=np.arange(len(pivot_tps.index)), labels=[str(s) for s in pivot_tps.index])
    plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "throughput_latency_pareto_heatmap.png"), dpi=300)
    plt.close()
    print("âœ… Pareto throughputâ€“latency heatmap saved: throughput_latency_pareto_heatmap.png")

    # === Plot 8: Scalability Gain (%) vs Number of Shards ===
    print("ðŸ“ˆ Calculating scalability gain relative to PoW baseline...")

    # Step 1: Collect baseline (PoW) and proposed (ML) throughputs
    gain_data = []
    shard_counts = [1, 2, 3, 5, 10]

    # Fix malicious fraction for fair comparison
    MAL_FRAC = 0.1

    for shard_count in shard_counts:
        #global NUM_SHARDS
        NUM_SHARDS = shard_count

        # Baseline PoW
        pow_meta = simulate_run(mode="pow", seed=0, malicious_frac=MAL_FRAC)
        # Proposed ML+Reputation
        ml_meta = simulate_run(mode="ml_reputation", seed=0, malicious_frac=MAL_FRAC)

        gain = ((ml_meta["tps"] - pow_meta["tps"]) / pow_meta["tps"]) * 100
        gain_data.append({
            "num_shards": shard_count,
            "pow_tps": pow_meta["tps"],
            "ml_tps": ml_meta["tps"],
            "latency_ml": ml_meta["latency"],
            "gain_percent": gain
        })

        print(f"ðŸ§© Shards={shard_count}: PoW={pow_meta['tps']:.2f} TPS | ML+Rep={ml_meta['tps']:.2f} TPS | Gain={gain:.2f}%")

    # Step 2: Save and plot
    gain_df = pd.DataFrame(gain_data)
    gain_df.to_csv(os.path.join(OUTDIR, "scalability_gain_comparison.csv"), index=False)

    # Step 3: Plot bar graph
    plt.figure(figsize=(8,6))
    plt.bar(gain_df["num_shards"], gain_df["gain_percent"], color="seagreen", alpha=0.8)
    for i, v in enumerate(gain_df["gain_percent"]):
        plt.text(gain_df["num_shards"][i], v + 5, f"{v:.1f}%", ha="center", fontweight="bold")
    plt.title("Scalability Gain (%) vs Number of Shards", fontsize=14, fontweight="bold")
    plt.xlabel("Number of Shards", fontsize=12)
    plt.ylabel("Performance Gain over PoW (%)", fontsize=12)
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTDIR, "scalability_gain_vs_shards.png"), dpi=300)
    plt.close()
    print("âœ… Scalability gain plot saved: scalability_gain_vs_shards.png")

    print("âœ… All plots saved in:", OUTDIR)

if __name__ == "__main__":
    run_batch_and_plot()

